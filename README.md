ğŸ§  DeepfakeBench: Deepfake Detection Benchmark
ğŸ“Œ Overview

DeepfakeBench is an AI-driven benchmarking framework designed to evaluate and compare deepfake detection models on manipulated facial media.
The project focuses on building a reproducible pipeline for dataset preprocessing, model training, evaluation, and inference, enabling systematic analysis of deepfake detection performance.

This repository serves as both a research-oriented benchmark and a practical implementation for real-world deepfake detection.

ğŸ¯ Objectives

1. Develop a standardized pipeline for deepfake detection benchmarking
2. Train and evaluate CNN-based deepfake classifiers on benchmark datasets
3. Compare model performance using robust evaluation metrics
4. Enable real-time inference through a user-friendly interface
5. Support reproducible research and extensibility

ğŸ—‚ï¸ Project Structure
DeepfakeBench/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ real/
â”‚   â””â”€â”€ fake/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cnn_model.py
â”‚   â””â”€â”€ xception_model.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_preprocessing.ipynb
â”‚   â””â”€â”€ model_training.ipynb
â”‚
â”œâ”€â”€ app.py                # Streamlit web application
â”œâ”€â”€ train.py              # Model training script
â”œâ”€â”€ evaluate.py           # Model evaluation script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ§ª Dataset
Primary Dataset: FaceForensics++
Contains both real and manipulated (deepfake) facial images/videos
Preprocessing includes:
Frame extraction from videos
Face detection and cropping
Image normalization and resizing
âš ï¸ Dataset files are not included in this repository due to size and licensing constraints.   

ğŸ§  Models Implemented
Convolutional Neural Networks (CNN)
Transfer Learning Models:
1. XceptionNet
2. ResNet (optional extension)

ğŸ“Š Evaluation Metrics
Accuracy
Precision
Recall
F1-score
Confusion Matrix
These metrics provide a balanced evaluation for imbalanced deepfake datasets.

ğŸš€ Features
Modular and scalable pipeline
Research-friendly benchmarking framework
Real-time inference using Streamlit
Clean separation of training, evaluation, and deployment
Easily extensible for new datasets and models

â–¶ï¸ How to Run
1ï¸âƒ£ Clone the Repository
git clone https://github.com/USERNAME/DeepfakeBench.git
cd DeepfakeBench

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Train the Model
python train.py

4ï¸âƒ£ Evaluate the Model
python evaluate.py

5ï¸âƒ£ Run the Web App
streamlit run app.py

ğŸ”¬ Research Contribution
Provides a reproducible benchmark for deepfake detection
Helps analyze model robustness against manipulated media
Can be extended to support multi-dataset benchmarking

ğŸ› ï¸ Future Enhancements
Support for additional datasets (DFDC, Celeb-DF)
Video-level deepfake classification
Attention-based and Transformer models
Explainable AI (Grad-CAM)
Model robustness testing against adversarial attacks

ğŸ‘©â€ğŸ’» Author
Akansha Srivastava
B.Tech Computer Science Engineering
Interested in AI, Cyber Security, and Applied Machine Learning

ğŸ“œ License
This project is intended for academic and research purposes.
