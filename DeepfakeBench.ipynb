{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c192fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python313\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: opencv-python in c:\\python313\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in c:\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: streamlit in c:\\python313\\lib\\site-packages (1.50.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python313\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python313\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\python313\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python313\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\python313\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python313\\lib\\site-packages (from tensorflow) (78.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\python313\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python313\\lib\\site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.2.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in c:\\python313\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\python313\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\python313\\lib\\site-packages (from streamlit) (6.2.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\python313\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\python313\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\python313\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\python313\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\python313\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\python313\\lib\\site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\python313\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\python313\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\python313\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python313\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: rich in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python313\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python313\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python scikit-learn matplotlib streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9a29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 14 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\000.mp4\n",
      "Extracted 16 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\001.mp4\n",
      "Extracted 24 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\002.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\003.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\004.mp4\n",
      "Extracted 13 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\005.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\006.mp4\n",
      "Extracted 17 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\007.mp4\n",
      "Extracted 22 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\008.mp4\n",
      "Extracted 20 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\009.mp4\n",
      "Extracted 16 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\010.mp4\n",
      "Extracted 22 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\011.mp4\n",
      "Extracted 13 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\012.mp4\n",
      "Extracted 12 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\013.mp4\n",
      "Extracted 23 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\014.mp4\n",
      "Extracted 15 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\015.mp4\n",
      "Extracted 23 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\016.mp4\n",
      "Extracted 14 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\017.mp4\n",
      "Extracted 15 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\018.mp4\n",
      "Extracted 47 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\019.mp4\n",
      "Extracted 14 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\000_003.mp4\n",
      "Extracted 16 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\001_870.mp4\n",
      "Extracted 24 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\002_006.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\003_000.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\004_982.mp4\n",
      "Extracted 13 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\005_010.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\006_002.mp4\n",
      "Extracted 17 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\007_132.mp4\n",
      "Extracted 22 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\008_990.mp4\n",
      "Extracted 20 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\009_027.mp4\n",
      "Extracted 16 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\010_005.mp4\n",
      "Extracted 22 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\011_805.mp4\n",
      "Extracted 12 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\985_981.mp4\n",
      "Extracted 28 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\986_994.mp4\n",
      "Extracted 20 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\987_938.mp4\n",
      "Extracted 16 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\988_966.mp4\n",
      "Extracted 32 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\989_993.mp4\n",
      "Extracted 11 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\990_008.mp4\n",
      "Extracted 14 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\991_064.mp4\n",
      "Extracted 24 frames from C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\992_980.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input video folders\n",
    "real_videos_path =r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\"\n",
    "fake_videos_path =r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\"\n",
    "\n",
    "# Output dataset folders\n",
    "output_real_path = \"dataset/real\"\n",
    "output_fake_path = \"dataset/fake\"\n",
    "\n",
    "# Create output dirs if not exist\n",
    "os.makedirs(output_real_path, exist_ok=True)\n",
    "os.makedirs(output_fake_path, exist_ok=True)\n",
    "\n",
    "def extract_frames(video_path, output_folder, prefix, frame_interval=30):\n",
    "    \"\"\"\n",
    "    Extract frames from video every `frame_interval` frames.\n",
    "    Saves them with video filename included.\n",
    "    \"\"\"\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]  # e.g., video1\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save every `frame_interval` frames\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = f\"{video_name}_{saved_count}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames from {video_path}\")\n",
    "\n",
    "# Process all real videos\n",
    "for file in os.listdir(real_videos_path):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        extract_frames(os.path.join(real_videos_path, file), output_real_path, \"real\")\n",
    "\n",
    "# Process all fake videos\n",
    "for file in os.listdir(fake_videos_path):\n",
    "    if file.endswith(\".mp4\"):\n",
    "        extract_frames(os.path.join(fake_videos_path, file), output_fake_path, \"fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d52b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python313\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python313\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python313\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\python313\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python313\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\python313\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python313\\lib\\site-packages (from tensorflow) (78.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\python313\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python313\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python313\\lib\\site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python313\\lib\\site-packages (from tensorflow) (2.2.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\python313\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\python313\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python313\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python313\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python313\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\python313\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python313\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python313\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python313\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Extracting frames...\n",
      "Found 400 real frames, 400 fake frames\n",
      "Found 640 validated image filenames.\n",
      "Found 160 validated image filenames.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6261 - loss: 0.6274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.6687 - loss: 0.5696 - val_accuracy: 0.7312 - val_loss: 0.5268\n",
      "Epoch 2/5\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7723 - loss: 0.4179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.7484 - loss: 0.4374 - val_accuracy: 0.7812 - val_loss: 0.4042\n",
      "Epoch 3/5\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8205 - loss: 0.3888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 1s/step - accuracy: 0.8000 - loss: 0.3749 - val_accuracy: 0.8250 - val_loss: 0.3029\n",
      "Epoch 4/5\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.8125 - loss: 0.3332 - val_accuracy: 0.8250 - val_loss: 0.4365\n",
      "Epoch 5/5\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.8266 - loss: 0.3439 - val_accuracy: 0.8125 - val_loss: 0.3272\n",
      " Training complete! Model saved as deepfake_xception_demo.h5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step\n",
      " Training metrics saved in 'plots/'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        REAL       0.82      0.80      0.81        80\n",
      "        FAKE       0.80      0.82      0.81        80\n",
      "\n",
      "    accuracy                           0.81       160\n",
      "   macro avg       0.81      0.81      0.81       160\n",
      "weighted avg       0.81      0.81      0.81       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # type: ignore\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Extract frames from videos\n",
    "# -----------------------------\n",
    "def extract_frames(video_path, output_dir, label, max_frames=20):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    saved = 0\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    while cap.isOpened() and saved < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % 10 == 0:  # sample every 10th frame\n",
    "            frame_path = os.path.join(output_dir, f\"{label}_{os.path.basename(video_path)}_{saved}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved += 1\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "def prepare_dataset(real_videos_path, fake_videos_path, frame_dir=\"frames\"):\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    all_frames = []\n",
    "    labels = []\n",
    "\n",
    "    # extract frames from real videos\n",
    "    for vid in os.listdir(real_videos_path):\n",
    "        vpath = os.path.join(real_videos_path, vid)\n",
    "        extract_frames(vpath, frame_dir, \"real\")\n",
    "    \n",
    "    # extract frames from fake videos\n",
    "    for vid in os.listdir(fake_videos_path):\n",
    "        vpath = os.path.join(fake_videos_path, vid)\n",
    "        extract_frames(vpath, frame_dir, \"fake\")\n",
    "\n",
    "    # collect all frames\n",
    "    for img in os.listdir(frame_dir):\n",
    "        if img.endswith(\".jpg\"):\n",
    "            all_frames.append(os.path.join(frame_dir, img))\n",
    "            labels.append(0 if \"real_\" in img else 1)  # 0=real, 1=fake\n",
    "\n",
    "    return all_frames, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Create generators\n",
    "# -----------------------------\n",
    "def create_generator(paths, labels, batch_size=8, is_train=True):\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        horizontal_flip=is_train,\n",
    "        rotation_range=10 if is_train else 0\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"filename\": paths,\n",
    "        \"label\": labels\n",
    "    })\n",
    "    \n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(150, 150),  # smaller for CPU\n",
    "        class_mode=\"raw\",        # labels are numeric\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_train\n",
    "    )\n",
    "    return generator, df\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define model\n",
    "# -----------------------------\n",
    "def build_model():\n",
    "    base = Xception(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=base.input, outputs=out)\n",
    "    model.compile(optimizer=Adam(1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Plot graphs + metrics\n",
    "# -----------------------------\n",
    "def plot_metrics(history, model, val_gen, val_df, output_dir=\"plots\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"accuracy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    val_gen.reset()\n",
    "    y_true = val_df[\"label\"].values\n",
    "    y_pred = model.predict(val_gen, verbose=1)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"red\", lw=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(output_dir, \"roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred_class = (y_pred > 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_class)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"REAL\", \"FAKE\"],\n",
    "                yticklabels=[\"REAL\", \"FAKE\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred_class, target_names=[\"REAL\", \"FAKE\"])\n",
    "    with open(os.path.join(output_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\" Training metrics saved in '{output_dir}/'\")\n",
    "    print(report)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main training\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    real_videos_path = r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\"   \n",
    "    fake_videos_path = r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\"\n",
    "\n",
    "    print(\"Extracting frames...\")\n",
    "    all_frames, labels = prepare_dataset(real_videos_path, fake_videos_path)\n",
    "    print(f\"Found {len([l for l in labels if l==0])} real frames, {len([l for l in labels if l==1])} fake frames\")\n",
    "\n",
    "    # train/val split\n",
    "    train_paths, val_paths, y_train, y_val = train_test_split(\n",
    "        all_frames, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # generators\n",
    "    train_gen, train_df = create_generator(train_paths, y_train, batch_size=8, is_train=True)\n",
    "    val_gen, val_df = create_generator(val_paths, y_val, batch_size=8, is_train=False)\n",
    "\n",
    "    # model\n",
    "    model = build_model()\n",
    "\n",
    "    # save best model\n",
    "    checkpoint = ModelCheckpoint(\"deepfake_xception_demo.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=[checkpoint])\n",
    "\n",
    "    print(\" Training complete! Model saved as deepfake_xception_demo.h5\")\n",
    "\n",
    "    # plot training graphs + metrics\n",
    "    plot_metrics(history, model, val_gen, val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc404473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.applications.xception import preprocess_input # type: ignore\n",
    "from tensorflow.keras.preprocessing import image # type: ignore\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"deepfake_xception_demo.h5\")\n",
    "print(\" Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7698975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frame(frame):\n",
    "    # Resize to 150x150 (same as training)\n",
    "    frame_resized = cv2.resize(frame, (150, 150))\n",
    "    img_array = image.img_to_array(frame_resized)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Prediction: output close to 0 = real, close to 1 = fake\n",
    "    pred = model.predict(img_array)[0][0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a797d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_path, max_frames=50):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    preds = []\n",
    "    count = 0\n",
    "    \n",
    "    while cap.isOpened() and count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        pred = predict_frame(frame)\n",
    "        preds.append(pred)\n",
    "        count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    avg_pred = np.mean(preds)\n",
    "    print(f\" Average prediction score: {avg_pred:.4f}\")\n",
    "    if avg_pred > 0.5:\n",
    "        print(\"DEEPFAKE\")\n",
    "    else:\n",
    "        print(\"REAL\")\n",
    "    return avg_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf70f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      " Average prediction score: 0.9984\n",
      "DEEPFAKE\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      " Average prediction score: 0.0092\n",
      "REAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.009155247)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace with one of your saved test videos\n",
    "video_path = r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\Deepfakes\\000_003.mp4\"\n",
    "predict_video(video_path, max_frames=30)\n",
    "\n",
    "video_path = r\"C:\\Users\\akans\\OneDrive\\Desktop\\FF++\\original\\012.mp4\"\n",
    "predict_video(video_path, max_frames=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
